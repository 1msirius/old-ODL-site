# Introduction to Large Language Models

Large language models (LLMs) are advanced neural networks trained on vast amounts of text data, comprising trillions of words to understand and generate human-like text.
These models demand significant computing power and usually undergo weeks or months of training.

After training, these foundational models excel in various natural language processing (NLP) tasks, including text generation, question answering, summarization, reasoning, and problem-solving, exhibiting remarkable fluency and coherence.

Models like GPT, LLama, and BERT have billions of parameters. You'll learn more about parameters later, but for now, imagine them as the model's memory. The more parameters a model has, the better its memory, and it can handle more complex tasks as a result.

## What is a Prompt

LLMs can understand and perform tasks based on natural language or human-written instructions, much like a human would. The input given to an LLM is called a prompt. The memory or space allocated for this prompt is known as the context window, which typically accommodates thousands of words, though this capacity varies across different models.

For example you asked the model, "_How far is the Moon from Earth?_" your inquiry is passed to the model, which then predicts the next words. Since your prompt is a question, the model generates an answer based on its understanding of the topic.

_Prompt:_

```md
How far is the Moon from Earth?
```

_Output:_

```md
On average, the Moon is about 384,400 kilometers (238,900 miles) away from Earth. However, this distance can vary slightly due to the elliptical shape of the Moon's orbit around Earth.
```

